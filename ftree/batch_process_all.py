#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import csv
import re
import json
import time
import urllib.request
import urllib.parse
import os

# Cookieé…ç½®
COOKIE = '.SpiderForum=39C3812FF342DDC067C65954D8248E8ED4C6254F5D172DC6BFD5ABB9BE0A06E959273DDFA1ECF55FA0F89C64165A0EDD5EAA7837219A676B5D1F7904C859328064345142A72DCDA78CAF9C17EBD1E720C8B321981C37A4C0334368B26F9FB94AE83E2E87BE293750A6B25051FA31A563FF2487D7909BA3346B0C6FDEFC90B7115947B07A77E27129ED522F3F9F5A618CE42A44E7023E01E196759020DB1277C11E56A7DD; Languages=; MemberId=7e9ae397-b609-45bb-a7cd-4a05f5f69b0c; UserEntityId=15276; UserName=%e5%8d%97%e6%98%8c%e8%b0%b1%e6%ba%90%e5%85%ac%e5%8f%b8; ASP.NET_SessionId=sdk1wdjpk4oxt345esezp1vl'

def fetch_person_page(url):
    """è·å–ä¸ªäººè¯¦æƒ…é¡µHTML"""
    try:
        # è§£æURLï¼Œå¯¹ä¸­æ–‡å‚æ•°è¿›è¡Œç¼–ç 
        if '?' in url:
            base_url, params_str = url.split('?', 1)
            params = {}
            for param in params_str.split('&'):
                if '=' in param:
                    key, value = param.split('=', 1)
                    params[key] = value
            
            # é‡æ–°ç¼–ç URL
            encoded_params = urllib.parse.urlencode(params, encoding='utf-8')
            url = f"{base_url}?{encoded_params}"
        
        headers = {
            'Cookie': COOKIE,
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/26.2 Safari/605.1.15'
        }
        
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=30) as response:
            html = response.read().decode('utf-8')
            return html
    except Exception as e:
        return None

def parse_person_details(html, nid, name_from_csv, note_from_csv=''):
    """è§£æä¸ªäººè¯¦æƒ…é¡µï¼Œæå–æ‰€æœ‰ä¿¡æ¯"""
    if not html:
        return None
    
    result = {
        'nid': nid,
        'name': name_from_csv,
        'generation': None,
        'url': None,
        'note': note_from_csv if note_from_csv else None,
        'father': None,
        'relation_to_father': None,
        'children': [],
        'siblings': None,
        'detail_info': {
            'raw_text': None,
            'aliases': {},
            'birth': None,
            'death': None,
            'burial': None,
            'migration': None,
            'description': None
        },
        'spouse': None
    }
    
    # 1. æå–ä¸–ä»£ä¿¡æ¯
    generation_match = re.search(r'ç¬¬(\d+)ä¸–</font>', html)
    if generation_match:
        result['generation'] = int(generation_match.group(1))
    
    # 2. æå–ä¸çˆ¶äº²çš„å…³ç³»
    relation_match = re.search(r'<font color=blue>([^<]+)</font>&nbsp;?([^&<]+?)&nbsp;?<font color=blue>' + re.escape(name_from_csv), html, re.DOTALL)
    if relation_match:
        relation = relation_match.group(2).strip()
        result['relation_to_father'] = relation if relation else None
    
    # 3. æå–çˆ¶äº²ä¿¡æ¯
    father_section = re.search(r'çˆ¶äº².*?<a[^>]+href="([^"]+)"[^>]*>.*?<font color=blue>([^<]+)</font>', html, re.DOTALL)
    if father_section:
        father_url = father_section.group(1)
        father_name = father_section.group(2).strip()
        father_nid_match = re.search(r'NId=([^&]+)', father_url)
        father_nid = father_nid_match.group(1) if father_nid_match else None
        
        result['father'] = {
            'nid': father_nid,
            'name': father_name,
            'url': f"http://112.5.13.209:88/wap/{father_url}"
        }
    
    # 4. æå–å­å¥³ä¿¡æ¯
    children_section = re.search(r'å­å¥³.*?</legend>(.*?)</fieldset>', html, re.DOTALL)
    if children_section:
        child_pattern = r'<a[^>]+href="([^"]+)"[^>]*>.*?<div class="CvChilName">([^<]+)</div>'
        child_matches = re.finditer(child_pattern, children_section.group(1), re.DOTALL)
        
        for match in child_matches:
            child_url = match.group(1).strip()
            child_full_name = match.group(2).strip()
            
            # æ¸…ç†å¯èƒ½çš„HTMLå®ä½“
            child_full_name = re.sub(r'&nbsp;?', '', child_full_name)
            
            # è§£æå…³ç³»å’Œå§“å
            child_name_match = re.match(r'(.*?)([\u4e00-\u9fa5]{1,4})$', child_full_name)
            if child_name_match:
                child_relation = child_name_match.group(1).strip()
                child_name = child_name_match.group(2).strip()
            else:
                child_relation = ''
                child_name = child_full_name
            
            child_nid_match = re.search(r'NId=([^&]+)', child_url)
            child_nid = child_nid_match.group(1) if child_nid_match else None
            
            result['children'].append({
                'nid': child_nid,
                'name': child_name,
                'relation': child_relation,
                'url': f"http://112.5.13.209:88/wap/{child_url}"
            })
    
    # 5. æå–å…„å¼Ÿå§å¦¹ä¿¡æ¯
    siblings_section = re.search(r'åœ¨å…„å¼Ÿå§å¦¹ä¸­æ’è¡Œ.*?</legend>(.*?)</fieldset>', html, re.DOTALL)
    if siblings_section:
        content = siblings_section.group(1)
        sibling_pattern = r'([\u4e00-\u9fa5]+å­)&nbsp<strong>([^<]+)</strong>'
        all_siblings = re.findall(sibling_pattern, content)
        
        # æ‰¾åˆ°æœ¬äººçš„æ’è¡Œ
        rank = None
        for i, (relation, name) in enumerate(all_siblings, 1):
            if name == name_from_csv or f'<font color=red>{relation}' in content:
                rank = i
                break
        
        result['siblings'] = {
            'total_count': len(all_siblings),
            'rank': rank,
            'list': [f"{rel} {name}" for rel, name in all_siblings]
        }
    
    # 6. æå–"å…¥è°±äººè¯¦ç»†ä¿¡æ¯"å®Œæ•´æ–‡æœ¬
    detail_section = re.search(r'å…¥è°±äººè¯¦ç»†ä¿¡æ¯.*?</legend>\s*(.*?)\s*<br\s*/>', html, re.DOTALL)
    if detail_section:
        raw_html = detail_section.group(1)
        detail_text = re.sub(r'<[^>]+>', ' ', raw_html)
        detail_text = re.sub(r'&nbsp;?', ' ', detail_text)
        detail_text = re.sub(r'\s+', ' ', detail_text).strip()
        result['detail_info']['raw_text'] = detail_text
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°å®Œæ•´å†…å®¹ï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…
        if len(detail_text) < 20:
            detail_section2 = re.search(r'PanelNamememo.*?</legend>(.*?)</fieldset>', html, re.DOTALL)
            if detail_section2:
                raw_html = detail_section2.group(1)
                detail_text = re.sub(r'<[^>]+>', ' ', raw_html)
                detail_text = re.sub(r'&nbsp;?', ' ', detail_text)
                detail_text = re.sub(r'\s+', ' ', detail_text).strip()
                detail_text = re.sub(r'å¤šåª’ä½“ç›¸å…³.*$', '', detail_text).strip()
                result['detail_info']['raw_text'] = detail_text
        
        # æå–ç»“æ„åŒ–ä¿¡æ¯
        if result['detail_info']['raw_text']:
            detail_text = result['detail_info']['raw_text']
            
            # æå–è®³
            hui_match = re.search(r'è®³([^\så·]+)', detail_text)
            if hui_match:
                result['detail_info']['aliases']['hui'] = hui_match.group(1)
            
            # æå–å·
            hao_match = re.search(r'å·([^\s]+)', detail_text)
            if hao_match:
                result['detail_info']['aliases']['hao'] = hao_match.group(1)
            
            # æå–å‡ºç”Ÿä¿¡æ¯
            birth_match = re.search(r'ç”Ÿäº\s*(.*?)\s*(?:æ®äº|è‘¬|å¨¶|ç”Ÿå­|$)', detail_text)
            if birth_match:
                result['detail_info']['birth'] = birth_match.group(1).strip()
            
            # æå–å»ä¸–ä¿¡æ¯
            death_match = re.search(r'æ®äº\s*(.*?)\s*(?:è‘¬|å¨¶|ç”Ÿå­|$)', detail_text)
            if death_match:
                result['detail_info']['death'] = death_match.group(1).strip()
            
            # æå–å®‰è‘¬ä¿¡æ¯
            burial_match = re.search(r'è‘¬(.*?)(?:å¨¶|ç”Ÿå­|é…|ç¬¬\d+ä¸–|$)', detail_text)
            if burial_match:
                result['detail_info']['burial'] = burial_match.group(1).strip()
            
            # æå–è¿å¾™ä¿¡æ¯
            migration_match = re.search(r'è¿([^\sã€‚]+)', detail_text)
            if migration_match:
                result['detail_info']['migration'] = migration_match.group(1).strip()
    
    # 7. æå–é…å¶ä¿¡æ¯
    spouse_section = re.search(r'é…å¶ä¿¡æ¯.*?</legend>(.*?)</fieldset>', html, re.DOTALL)
    if spouse_section:
        spouse_html = spouse_section.group(1)
        spouse_text = re.sub(r'<[^>]+>', ' ', spouse_html)
        spouse_text = re.sub(r'\s+', ' ', spouse_text).strip()
        
        if spouse_text:
            spouse_name_match = re.search(r'å¨¶([^\sæ°]+æ°)', result['detail_info']['raw_text'] or '')
            result['spouse'] = {
                'name': spouse_name_match.group(1) if spouse_name_match else None,
                'detail': spouse_text if spouse_text else None
            }
    
    return result

def main():
    """å¤„ç†å…¨éƒ¨673æ¡æ•°æ®"""
    print("="*80)
    print("å¼€å§‹å¤„ç†æ—è°±æ•°æ®ï¼ˆå…¨éƒ¨673æ¡ï¼‰".center(76))
    print("="*80)
    
    csv_file = '/Users/qixxu01/Downloads/family_basic_info.csv'
    output_file = '/Users/qixxu01/Downloads/family_tree_full2.json'
    progress_file = '/Users/qixxu01/Downloads/family_tree_progress.json'
    
    # è¯»å–CSVæ–‡ä»¶
    with open(csv_file, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        all_records = list(reader)
    
    print(f"\nğŸ“Š å…±è¯»å– {len(all_records)} æ¡è®°å½•")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¿›åº¦æ–‡ä»¶
    results = []
    start_index = 0
    if os.path.exists(progress_file):
        print(f"ğŸ“ å‘ç°è¿›åº¦æ–‡ä»¶ï¼Œç»§ç»­ä¸Šæ¬¡çš„å¤„ç†...")
        with open(progress_file, 'r', encoding='utf-8') as f:
            saved_data = json.load(f)
            results = saved_data['results']
            start_index = saved_data['last_index'] + 1
            print(f"âœ… å·²åŠ è½½ {len(results)} æ¡è®°å½•ï¼Œä»ç¬¬ {start_index + 1} æ¡ç»§ç»­\n")
    else:
        print(f"ğŸ†• å¼€å§‹å…¨æ–°å¤„ç†\n")
    
    # ç»Ÿè®¡ä¿¡æ¯
    success_count = len(results)
    fail_count = 0
    
    try:
        for i in range(start_index, len(all_records)):
            record = all_records[i]
            name = record['å§“å']
            url = record['URL']
            nid = record['NID']
            note = record.get('å¤‡æ³¨', '')
            
            print(f"[{i+1}/{len(all_records)}] {name} (NID: {nid[:8]}...)", end=' ')
            
            # è·å–è¯¦æƒ…é¡µ
            html = fetch_person_page(url)
            if not html:
                print(f"âŒ è·å–å¤±è´¥")
                fail_count += 1
                continue
            
            # è§£ææ•°æ®
            person_data = parse_person_details(html, nid, name, note)
            if person_data:
                results.append(person_data)
                success_count += 1
                print(f"âœ… ä¸–ä»£:{person_data['generation']} çˆ¶:{person_data['father']['name'] if person_data['father'] else 'æ— '} å­:{len(person_data['children'])}äºº")
            else:
                print(f"âŒ è§£æå¤±è´¥")
                fail_count += 1
            
            # æ¯10æ¡ä¿å­˜ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 10 == 0:
                with open(progress_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'results': results,
                        'last_index': i,
                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                    }, f, ensure_ascii=False, indent=2)
                print(f"    ğŸ’¾ å·²ä¿å­˜è¿›åº¦ ({success_count}æˆåŠŸ/{fail_count}å¤±è´¥)")
            
            # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.3)
    
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼æ­£åœ¨ä¿å­˜å½“å‰è¿›åº¦...")
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump({
                'results': results,
                'last_index': i,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }, f, ensure_ascii=False, indent=2)
        print(f"âœ… è¿›åº¦å·²ä¿å­˜åˆ°: {progress_file}")
        print(f"ğŸ“Š å·²å¤„ç†: {len(results)} æ¡è®°å½•")
        return
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # åˆ é™¤è¿›åº¦æ–‡ä»¶
    if os.path.exists(progress_file):
        os.remove(progress_file)
    
    print(f"\n{'='*80}")
    print(f"âœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ å·²ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š æˆåŠŸ: {success_count} æ¡")
    print(f"ğŸ“Š å¤±è´¥: {fail_count} æ¡")
    print(f"ğŸ“Š æ€»è®¡: {len(all_records)} æ¡")
    print(f"{'='*80}\n")

if __name__ == '__main__':
    main()
