#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import csv
import re
import json
import time
import urllib.request
import urllib.parse
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Cookieé…ç½®
COOKIE = '.SpiderForum=39C3812FF342DDC067C65954D8248E8ED4C6254F5D172DC6BFD5ABB9BE0A06E959273DDFA1ECF55FA0F89C64165A0EDD5EAA7837219A676B5D1F7904C859328064345142A72DCDA78CAF9C17EBD1E720C8B321981C37A4C0334368B26F9FB94AE83E2E87BE293750A6B25051FA31A563FF2487D7909BA3346B0C6FDEFC90B7115947B07A77E27129ED522F3F9F5A618CE42A44E7023E01E196759020DB1277C11E56A7DD; Languages=; MemberId=7e9ae397-b609-45bb-a7cd-4a05f5f69b0c; UserEntityId=15276; UserName=%e5%8d%97%e6%98%8c%e8%b0%b1%e6%ba%90%e5%85%ac%e5%8f%b8; ASP.NET_SessionId=sdk1wdjpk4oxt345esezp1vl'

# çº¿ç¨‹é”ç”¨äºæ‰“å°è¾“å‡º
print_lock = threading.Lock()

def thread_print(*args, **kwargs):
    """çº¿ç¨‹å®‰å…¨çš„æ‰“å°å‡½æ•°"""
    with print_lock:
        print(*args, **kwargs)

def fetch_person_page(url):
    """è·å–ä¸ªäººè¯¦æƒ…é¡µHTML"""
    try:
        # è§£æURLï¼Œå¯¹ä¸­æ–‡å‚æ•°è¿›è¡Œç¼–ç 
        if '?' in url:
            base_url, params_str = url.split('?', 1)
            params = {}
            for param in params_str.split('&'):
                if '=' in param:
                    key, value = param.split('=', 1)
                    params[key] = value
            
            # é‡æ–°ç¼–ç URL
            encoded_params = urllib.parse.urlencode(params, encoding='utf-8')
            url = f"{base_url}?{encoded_params}"
        
        headers = {
            'Cookie': COOKIE,
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/26.2 Safari/605.1.15'
        }
        
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=30) as response:
            html = response.read().decode('utf-8')
            return html
    except Exception as e:
        return None

def parse_person_details(html, nid, name_from_csv, note_from_csv=''):
    """è§£æä¸ªäººè¯¦æƒ…é¡µï¼Œæå–æ‰€æœ‰ä¿¡æ¯"""
    if not html:
        return None
    
    result = {
        'nid': nid,
        'name': name_from_csv,
        'generation': None,
        'url': None,
        'note': note_from_csv if note_from_csv else None,
        'father': None,
        'relation_to_father': None,
        'children': [],
        'siblings': None,
        'detail_info': {
            'raw_text': None,
            'aliases': {},
            'birth': None,
            'death': None,
            'burial': None,
            'migration': None,
            'description': None
        },
        'spouse': None
    }
    
    # 1. æå–ä¸–ä»£ä¿¡æ¯
    generation_match = re.search(r'ç¬¬(\d+)ä¸–</font>', html)
    if generation_match:
        result['generation'] = int(generation_match.group(1))
    
    # 2. æå–ä¸çˆ¶äº²çš„å…³ç³»
    relation_match = re.search(r'<font color=blue>([^<]+)</font>&nbsp;?([^&<]+?)&nbsp;?<font color=blue>' + re.escape(name_from_csv), html, re.DOTALL)
    if relation_match:
        relation = relation_match.group(2).strip()
        result['relation_to_father'] = relation if relation else None
    
    # 3. æå–çˆ¶äº²ä¿¡æ¯
    father_section = re.search(r'çˆ¶äº².*?<a[^>]+href="([^"]+)"[^>]*>.*?<font color=blue>([^<]+)</font>', html, re.DOTALL)
    if father_section:
        father_url = father_section.group(1)
        father_name = father_section.group(2).strip()
        father_nid_match = re.search(r'NId=([^&]+)', father_url)
        father_nid = father_nid_match.group(1) if father_nid_match else None
        
        result['father'] = {
            'nid': father_nid,
            'name': father_name,
            'url': f"http://112.5.13.209:88/wap/{father_url}"
        }
    
    # 4. æå–å­å¥³ä¿¡æ¯
    children_section = re.search(r'å­å¥³.*?</legend>(.*?)</fieldset>', html, re.DOTALL)
    if children_section:
        child_pattern = r'<a[^>]+href="([^"]+)"[^>]*>.*?<div class="CvChilName">([^<]+)</div>'
        child_matches = re.finditer(child_pattern, children_section.group(1), re.DOTALL)
        
        for match in child_matches:
            child_url = match.group(1).strip()
            child_full_name = match.group(2).strip()
            
            # æ¸…ç†å¯èƒ½çš„HTMLå®ä½“
            child_full_name = re.sub(r'&nbsp;?', '', child_full_name)
            
            # è§£æå…³ç³»å’Œå§“å
            child_name_match = re.match(r'(.*?)([\u4e00-\u9fa5]{1,4})$', child_full_name)
            if child_name_match:
                child_relation = child_name_match.group(1).strip()
                child_name = child_name_match.group(2).strip()
            else:
                child_relation = ''
                child_name = child_full_name
            
            child_nid_match = re.search(r'NId=([^&]+)', child_url)
            child_nid = child_nid_match.group(1) if child_nid_match else None
            
            result['children'].append({
                'nid': child_nid,
                'name': child_name,
                'relation': child_relation,
                'url': f"http://112.5.13.209:88/wap/{child_url}"
            })
    
    # 5. æå–å…„å¼Ÿå§å¦¹ä¿¡æ¯
    siblings_section = re.search(r'åœ¨å…„å¼Ÿå§å¦¹ä¸­æ’è¡Œ.*?</legend>(.*?)</fieldset>', html, re.DOTALL)
    if siblings_section:
        content = siblings_section.group(1)
        sibling_pattern = r'([\u4e00-\u9fa5]+å­)&nbsp<strong>([^<]+)</strong>'
        all_siblings = re.findall(sibling_pattern, content)
        
        # æ‰¾åˆ°æœ¬äººçš„æ’è¡Œ
        rank = None
        for i, (relation, name) in enumerate(all_siblings, 1):
            if name == name_from_csv or f'<font color=red>{relation}' in content:
                rank = i
                break
        
        result['siblings'] = {
            'total_count': len(all_siblings),
            'rank': rank,
            'list': [f"{rel} {name}" for rel, name in all_siblings]
        }
    
    # 6. æå–"å…¥è°±äººè¯¦ç»†ä¿¡æ¯"å®Œæ•´æ–‡æœ¬
    detail_section = re.search(r'å…¥è°±äººè¯¦ç»†ä¿¡æ¯.*?</legend>\s*(.*?)\s*<br\s*/>', html, re.DOTALL)
    if detail_section:
        raw_html = detail_section.group(1)
        detail_text = re.sub(r'<[^>]+>', ' ', raw_html)
        detail_text = re.sub(r'&nbsp;?', ' ', detail_text)
        detail_text = re.sub(r'\s+', ' ', detail_text).strip()
        result['detail_info']['raw_text'] = detail_text
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°å®Œæ•´å†…å®¹ï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…
        if len(detail_text) < 20:
            detail_section2 = re.search(r'PanelNamememo.*?</legend>(.*?)</fieldset>', html, re.DOTALL)
            if detail_section2:
                raw_html = detail_section2.group(1)
                detail_text = re.sub(r'<[^>]+>', ' ', raw_html)
                detail_text = re.sub(r'&nbsp;?', ' ', detail_text)
                detail_text = re.sub(r'\s+', ' ', detail_text).strip()
                detail_text = re.sub(r'å¤šåª’ä½“ç›¸å…³.*$', '', detail_text).strip()
                result['detail_info']['raw_text'] = detail_text
        
        # æå–ç»“æ„åŒ–ä¿¡æ¯
        if result['detail_info']['raw_text']:
            detail_text = result['detail_info']['raw_text']
            
            # æå–è®³
            hui_match = re.search(r'è®³([^\så·]+)', detail_text)
            if hui_match:
                result['detail_info']['aliases']['hui'] = hui_match.group(1)
            
            # æå–å·
            hao_match = re.search(r'å·([^\s]+)', detail_text)
            if hao_match:
                result['detail_info']['aliases']['hao'] = hao_match.group(1)
            
            # æå–å‡ºç”Ÿä¿¡æ¯
            birth_match = re.search(r'ç”Ÿäº\s*(.*?)\s*(?:æ®äº|è‘¬|å¨¶|ç”Ÿå­|$)', detail_text)
            if birth_match:
                result['detail_info']['birth'] = birth_match.group(1).strip()
            
            # æå–å»ä¸–ä¿¡æ¯
            death_match = re.search(r'æ®äº\s*(.*?)\s*(?:è‘¬|å¨¶|ç”Ÿå­|$)', detail_text)
            if death_match:
                result['detail_info']['death'] = death_match.group(1).strip()
            
            # æå–å®‰è‘¬ä¿¡æ¯
            burial_match = re.search(r'è‘¬(.*?)(?:å¨¶|ç”Ÿå­|é…|ç¬¬\d+ä¸–|$)', detail_text)
            if burial_match:
                result['detail_info']['burial'] = burial_match.group(1).strip()
            
            # æå–è¿å¾™ä¿¡æ¯
            migration_match = re.search(r'è¿([^\sã€‚]+)', detail_text)
            if migration_match:
                result['detail_info']['migration'] = migration_match.group(1).strip()
    
    # 7. æå–é…å¶ä¿¡æ¯
    spouse_section = re.search(r'é…å¶ä¿¡æ¯.*?</legend>(.*?)</fieldset>', html, re.DOTALL)
    if spouse_section:
        spouse_html = spouse_section.group(1)
        spouse_text = re.sub(r'<[^>]+>', ' ', spouse_html)
        spouse_text = re.sub(r'\s+', ' ', spouse_text).strip()
        
        if spouse_text:
            spouse_name_match = re.search(r'å¨¶([^\sæ°]+æ°)', result['detail_info']['raw_text'] or '')
            result['spouse'] = {
                'name': spouse_name_match.group(1) if spouse_name_match else None,
                'detail': spouse_text if spouse_text else None
            }
    
    return result

def find_all_csv_files(base_dir):
    """é€’å½’æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶"""
    csv_files = []
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.endswith('.csv'):
                csv_path = os.path.join(root, file)
                csv_files.append(csv_path)
    return csv_files

def process_single_record(record, index, total):
    """å¤„ç†å•æ¡è®°å½•ï¼ˆä¾›çº¿ç¨‹æ± è°ƒç”¨ï¼‰"""
    name = record.get('å§“å', '')
    url = record.get('URL', '')
    nid = record.get('NID', '')
    note = record.get('å¤‡æ³¨', '')
    
    if not url or not nid:
        return None, f"[{index}/{total}] {name} âš ï¸ è·³è¿‡ï¼ˆæ— URLæˆ–NIDï¼‰"
    
    # è·å–è¯¦æƒ…é¡µ
    html = fetch_person_page(url)
    if not html:
        return None, f"[{index}/{total}] {name} âŒ"
    
    # è§£ææ•°æ®
    person_data = parse_person_details(html, nid, name, note)
    if person_data:
        return person_data, f"[{index}/{total}] {name} âœ…"
    else:
        return None, f"[{index}/{total}] {name} âŒ"

def process_csv_to_json(csv_file, max_workers=10):
    """å¤„ç†å•ä¸ªCSVæ–‡ä»¶ï¼Œç”Ÿæˆå¯¹åº”çš„JSONæ–‡ä»¶ï¼ˆä½¿ç”¨å¤šçº¿ç¨‹ï¼‰"""
    json_file = csv_file.replace('.csv', '.json')
    progress_file = csv_file.replace('.csv', '_progress.json')
    
    try:
        # è¯»å–CSVæ–‡ä»¶
        thread_print(f"  ğŸ“– è¯»å–CSVæ–‡ä»¶...")
        with open(csv_file, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            all_records = list(reader)
        thread_print(f"  âœ“ è¯»å–æˆåŠŸï¼Œå…± {len(all_records)} æ¡è®°å½•")
    except Exception as e:
        thread_print(f"  âŒ è¯»å–CSVå¤±è´¥: {e}")
        # å³ä½¿è¯»å–å¤±è´¥ï¼Œä¹Ÿå°è¯•åˆ›å»ºä¸€ä¸ªç©ºJSON
        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump([], f, ensure_ascii=False, indent=2)
            thread_print(f"  âš ï¸  å·²åˆ›å»ºç©ºJSONæ–‡ä»¶")
        except:
            pass
        return False
    
    if not all_records:
        thread_print(f"  âš ï¸  CSVæ–‡ä»¶ä¸ºç©ºï¼Œåˆ›å»ºç©ºJSON")
        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump([], f, ensure_ascii=False, indent=2)
            thread_print(f"  âœ… å·²åˆ›å»ºç©ºJSON: {os.path.basename(json_file)}")
            return True
        except Exception as e:
            thread_print(f"  âŒ åˆ›å»ºJSONå¤±è´¥: {e}")
            return False
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¿›åº¦æ–‡ä»¶
    results = []
    processed_nids = set()
    start_index = 0
    
    if os.path.exists(progress_file):
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
                results = progress_data.get('results', [])
                processed_nids = set(r['nid'] for r in results if r.get('nid'))
                thread_print(f"  â†» å‘ç°è¿›åº¦æ–‡ä»¶ï¼Œå·²å®Œæˆ {len(results)} æ¡è®°å½•ï¼Œç»§ç»­å¤„ç†...")
        except Exception as e:
            thread_print(f"  âš ï¸  è¿›åº¦æ–‡ä»¶æŸåï¼Œä»å¤´å¼€å§‹: {e}")
            results = []
            processed_nids = set()
    
    # è¿‡æ»¤å‡ºæœªå¤„ç†çš„è®°å½•
    pending_records = []
    for i, record in enumerate(all_records):
        nid = record.get('NID', '')
        if nid and nid in processed_nids:
            continue  # è·³è¿‡å·²å¤„ç†çš„
        pending_records.append((i, record))
    
    if not pending_records:
        thread_print(f"  âœ… æ‰€æœ‰è®°å½•éƒ½å·²å¤„ç†ï¼Œä¿å­˜æœ€ç»ˆç»“æœ...")
        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            if os.path.exists(progress_file):
                os.remove(progress_file)
            thread_print(f"  âœ… å®Œæˆ: {len(results)} æ¡è®°å½• â†’ {os.path.basename(json_file)}")
            return True
        except Exception as e:
            thread_print(f"  âŒ ä¿å­˜å¤±è´¥: {e}")
            return False
    
    thread_print(f"  ğŸ”„ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¤„ç†å‰©ä½™ {len(pending_records)} æ¡è®°å½•...")
    
    success_count = len(results)  # ä¹‹å‰å·²å®Œæˆçš„
    fail_count = 0
    skip_count = 0
    new_success = 0
    
    # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤resultsåˆ—è¡¨
    results_lock = threading.Lock()
    
    def save_progress():
        """ä¿å­˜è¿›åº¦æ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        try:
            with results_lock:
                with open(progress_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'results': results,
                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                        'total': len(all_records),
                        'processed': len(results) + fail_count + skip_count
                    }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            thread_print(f"    âš ï¸  ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    try:
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_record = {
                executor.submit(process_single_record, record, idx+1, len(all_records)): (idx, record)
                for idx, record in pending_records
            }
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_record):
                try:
                    person_data, msg = future.result()
                    thread_print(f"    {msg}")
                    
                    # çº¿ç¨‹å®‰å…¨åœ°æ·»åŠ ç»“æœ
                    with results_lock:
                        if person_data:
                            results.append(person_data)
                            new_success += 1
                        else:
                            if "è·³è¿‡" in msg:
                                skip_count += 1
                            else:
                                fail_count += 1
                    
                    completed += 1
                    
                    # æ¯10æ¡ä¿å­˜ä¸€æ¬¡è¿›åº¦
                    if completed % 10 == 0:
                        save_progress()
                    
                    # å°å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(0.05)
                except Exception as e:
                    thread_print(f"    âŒ å¤„ç†å¼‚å¸¸: {e}")
                    with results_lock:
                        fail_count += 1
        
        thread_print(f"  ğŸ“Š æœ¬æ¬¡å¤„ç†: {new_success}æˆåŠŸ {fail_count}å¤±è´¥ {skip_count}è·³è¿‡")
    
    except KeyboardInterrupt:
        thread_print(f"\n  âš ï¸  ç”¨æˆ·ä¸­æ–­ï¼ç­‰å¾…æ­£åœ¨å¤„ç†çš„çº¿ç¨‹å®Œæˆ...")
        
        # ç­‰å¾…æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡å®Œæˆï¼ˆæœ€å¤šç­‰å¾…30ç§’ï¼‰
        executor.shutdown(wait=True, cancel_futures=False)
        
        thread_print(f"  âœ“ çº¿ç¨‹å·²åœæ­¢ï¼Œå·²å¤„ç† {new_success} æ¡æ–°è®°å½•")
        
        # ä¿å­˜è¿›åº¦
        save_progress()
        thread_print(f"  ğŸ’¾ è¿›åº¦å·²ä¿å­˜åˆ°: {os.path.basename(progress_file)}")
        
        # ä¸è¦re-raiseï¼Œç»§ç»­ä¿å­˜å½“å‰ç»“æœåˆ°JSON
    except Exception as e:
        thread_print(f"  âŒ çº¿ç¨‹æ± å¼‚å¸¸: {e}")
        import traceback
        thread_print(traceback.format_exc())
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    thread_print(f"  ğŸ’¾ ä¿å­˜JSONæ–‡ä»¶: {os.path.basename(json_file)}")
    try:
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        total_success = len(results)
        thread_print(f"  âœ… å·²ä¿å­˜: {total_success} æ¡è®°å½• â†’ {os.path.basename(json_file)}")
        
        # åªæœ‰å…¨éƒ¨å®Œæˆæ‰åˆ é™¤è¿›åº¦æ–‡ä»¶
        if len(results) + fail_count + skip_count >= len(all_records):
            if os.path.exists(progress_file):
                os.remove(progress_file)
                thread_print(f"  ğŸ—‘ï¸  è¿›åº¦æ–‡ä»¶å·²åˆ é™¤")
        else:
            thread_print(f"  ğŸ’¡ æç¤º: é‡æ–°è¿è¡Œå¯ç»§ç»­å¤„ç†æ­¤æ–‡ä»¶çš„å‰©ä½™è®°å½•")
        
        return True
    except Exception as e:
        thread_print(f"  âŒ ä¿å­˜JSONå¤±è´¥: {e}")
        import traceback
        thread_print(traceback.format_exc())
        return False

def main():
    """éå†parsed_genealogyç›®å½•ï¼Œå°†æ‰€æœ‰CSVè½¬æ¢ä¸ºJSONï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰"""
    print("="*80)
    print("æ‰¹é‡å¤„ç† parsed_genealogy ç›®å½•ä¸‹çš„CSVæ–‡ä»¶ï¼ˆå¤šçº¿ç¨‹ï¼‰".center(76))
    print("="*80)
    
    base_dir = 'parsed_genealogy'
    max_workers = 5  # æ¯ä¸ªCSVæ–‡ä»¶å†…éƒ¨ä½¿ç”¨5ä¸ªçº¿ç¨‹
    
    if not os.path.exists(base_dir):
        print(f"\nâŒ é”™è¯¯: ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        return
    
    # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = find_all_csv_files(base_dir)
    
    if not csv_files:
        print(f"\nâš ï¸  åœ¨ {base_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
        return
    
    # æ£€æŸ¥å·²å®Œæˆçš„æ–‡ä»¶ï¼ˆæ—¢æ²¡æœ‰JSONä¹Ÿæ²¡æœ‰è¿›åº¦æ–‡ä»¶çš„æ‰æ˜¯å¾…å¤„ç†ï¼‰
    completed_files = []
    pending_files = []
    partial_files = []
    
    for csv_file in csv_files:
        json_file = csv_file.replace('.csv', '.json')
        progress_file = csv_file.replace('.csv', '_progress.json')
        
        if os.path.exists(json_file) and not os.path.exists(progress_file):
            # æœ‰JSONä¸”æ²¡æœ‰è¿›åº¦æ–‡ä»¶ï¼Œè¯´æ˜å·²å®Œå…¨å®Œæˆ
            completed_files.append(csv_file)
        elif os.path.exists(progress_file):
            # æœ‰è¿›åº¦æ–‡ä»¶ï¼Œè¯´æ˜ä¸­æ–­è¿‡ï¼Œéœ€è¦ç»§ç»­å¤„ç†
            partial_files.append(csv_file)
        else:
            # æ—¢æ²¡æœ‰JSONä¹Ÿæ²¡æœ‰è¿›åº¦æ–‡ä»¶ï¼Œæ˜¯å…¨æ–°çš„
            pending_files.append(csv_file)
    
    # å°†éƒ¨åˆ†å®Œæˆçš„æ–‡ä»¶æ”¾åˆ°å¾…å¤„ç†åˆ—è¡¨å‰é¢
    all_pending = partial_files + pending_files
    
    print(f"\nğŸ“Š æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    if completed_files:
        print(f"âœ… å·²å®Œæˆ {len(completed_files)} ä¸ªæ–‡ä»¶ï¼ˆå°†è·³è¿‡ï¼‰")
    if partial_files:
        print(f"â¸ï¸  éƒ¨åˆ†å®Œæˆ {len(partial_files)} ä¸ªæ–‡ä»¶ï¼ˆå°†ç»§ç»­å¤„ç†ï¼‰")
    if pending_files:
        print(f"â³ å¾…å¤„ç† {len(pending_files)} ä¸ªæ–‡ä»¶")
    
    if not all_pending:
        print(f"\nğŸ‰ æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†å®Œæˆï¼")
        return
    
    print(f"âš¡ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†æ¯ä¸ªæ–‡ä»¶")
    print("="*80)
    
    # é¡ºåºå¤„ç†æ¯ä¸ªCSVæ–‡ä»¶ï¼ˆä½†æ–‡ä»¶å†…éƒ¨ä½¿ç”¨å¤šçº¿ç¨‹ï¼‰
    success_files = 0
    fail_files = 0
    
    try:
        for idx, csv_file in enumerate(all_pending, 1):
            # ç›¸å¯¹è·¯å¾„æ˜¾ç¤º
            rel_path = os.path.relpath(csv_file, base_dir)
            thread_print(f"\n[{idx}/{len(all_pending)}] {rel_path}")
            
            try:
                # å¤„ç†CSVï¼ˆå†…éƒ¨ä½¿ç”¨5ä¸ªçº¿ç¨‹ï¼‰
                if process_csv_to_json(csv_file, max_workers=max_workers):
                    success_files += 1
                else:
                    fail_files += 1
            except KeyboardInterrupt:
                # ä»å†…å±‚ä¼ ä¸Šæ¥çš„ä¸­æ–­ï¼Œç›´æ¥é‡æ–°æŠ›å‡º
                raise
            except Exception as e:
                thread_print(f"  âŒ å¤„ç†å¼‚å¸¸: {e}")
                import traceback
                thread_print(traceback.format_exc())
                fail_files += 1
    
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼")
        print(f"ğŸ“Š æœ¬æ¬¡æˆåŠŸ: {success_files} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“Š æœ¬æ¬¡å¤±è´¥: {fail_files} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“Š å‰©ä½™æœªå¤„ç†: {len(all_pending) - success_files - fail_files} ä¸ªæ–‡ä»¶")
        print(f"\nğŸ’¡ æç¤º: é‡æ–°è¿è¡Œæ­¤è„šæœ¬å°†ä»ä¸­æ–­å¤„ç»§ç»­ï¼ˆæ”¯æŒæ–‡ä»¶å†…æ–­ç‚¹ç»­ä¼ ï¼‰")
        return
    
    print(f"\n{'='*80}")
    print(f"âœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š æœ¬æ¬¡æˆåŠŸ: {success_files}/{len(all_pending)} ä¸ªæ–‡ä»¶")
    if fail_files > 0:
        print(f"ğŸ“Š æœ¬æ¬¡å¤±è´¥: {fail_files}/{len(all_pending)} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“Š æ€»è®¡å®Œæˆ: {len(completed_files) + success_files}/{len(csv_files)} ä¸ªæ–‡ä»¶")
    print(f"{'='*80}\n")

if __name__ == '__main__':
    main()
